{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44955c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:17:01,754 - WARNING - `llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n",
      "2025-11-25 13:17:01,758 - WARNING - `llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n",
      "2025-11-25 13:17:04,356 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:17:05,427 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:17:07,120 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitting at someone else's table in the student center without asking angers MIT students.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What angers MIT students?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb798cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from ./storage...\n",
      "Index loaded successfully.\n",
      "2.007 and 2.009 type classes are considered the most controversial classes at MIT.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "\n",
    "# Define the storage directory. Ensure Google Drive is mounted if using a path in /content/drive\n",
    "STORAGE_DIR = \"./storage\"\n",
    "\n",
    "\n",
    "index = None # Initialize index to None\n",
    "\n",
    "try:\n",
    "    # 1. Check if the index has already been saved to disk\n",
    "    if not os.path.exists(STORAGE_DIR):\n",
    "        print(\"Index not found. Creating and persisting index...\")\n",
    "        # Ensure the directory exists before attempting to write to it\n",
    "        os.makedirs(STORAGE_DIR, exist_ok=True)\n",
    "        documents = SimpleDirectoryReader(\"sample_data\").load_data()\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        index.storage_context.persist(persist_dir=STORAGE_DIR)\n",
    "        print(f\"Index created and saved to {STORAGE_DIR}\")\n",
    "    else:\n",
    "        # 2. If the index exists, try to load it from the disk\n",
    "        print(f\"Loading index from {STORAGE_DIR}...\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(\"Index loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch any exception during loading, particularly JSONDecodeError\n",
    "    print(f\"Error loading index: {e}\")\n",
    "    if os.path.exists(STORAGE_DIR):\n",
    "        print(f\"Attempting to recreate index by clearing {STORAGE_DIR}...\")\n",
    "        shutil.rmtree(STORAGE_DIR) # Remove the corrupted directory\n",
    "        os.makedirs(STORAGE_DIR, exist_ok=True) # Recreate empty directory\n",
    "\n",
    "    print(\"Recreating and persisting index from scratch...\")\n",
    "    documents = SimpleDirectoryReader(\"sample_data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=STORAGE_DIR)\n",
    "    print(f\"Index recreated and saved to {STORAGE_DIR}\")\n",
    "\n",
    "\n",
    "# 3. Proceed with querying (This step uses the index whether it was created or loaded)\n",
    "if index: # Only proceed if index was successfully created or loaded\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(\"What are the most controversial classes at MIT?\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Failed to initialize index, cannot proceed with querying.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534b3ad",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01eb6a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 13:37:34,901 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 13:37:36,663 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#49647 After pulling an all-nighter, I walked toward MIT ambulance thinking it is a food truck\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What's the funniest MIT confession?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4645e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students at MIT have varying experiences with Course 6. Some students, particularly those with more experience or a competitive high school background, may find the course challenging due to the wide range of skill levels in the class. The workload and the perceived difficulty level can differ among students, leading to mixed feelings about the course.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do students feel about course 6 at MIT?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16091a",
   "metadata": {},
   "source": [
    "# Distributional Pluralism Engine\n",
    "\n",
    "The standard RAG approach above **averages** across retrieved documents, producing consensus responses like:\n",
    "> \"Students at MIT have varying experiences... some students... mixed feelings...\"\n",
    "\n",
    "Below we implement **distributional pluralism**: instead of blending perspectives, we randomly sample ONE confession and generate a response that embodies that single authentic voice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60da6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Distributional Query Engine initialized\n",
      "  - Retrieval width: top_k=30\n",
      "  - LLM: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Distributional Pluralism Query Engine\n",
    "# =============================================================================\n",
    "\n",
    "import random\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class DistributionalQueryEngine:\n",
    "    \"\"\"\n",
    "    A query engine that implements distributional pluralism.\n",
    "    \n",
    "    Instead of averaging across all retrieved documents, it:\n",
    "    1. Retrieves a wide net of relevant confessions (top_k=30)\n",
    "    2. Randomly samples ONE confession from the retrieved set\n",
    "    3. Generates a response that embodies that single perspective\n",
    "    \"\"\"\n",
    "    \n",
    "    # System prompt that instructs the LLM to embody ONE voice\n",
    "    SINGLE_VOICE_PROMPT = PromptTemplate(\n",
    "        \"\"\"You are an MIT student. The confession below reflects your genuine experience and perspective.\n",
    "\n",
    "CONFESSION:\n",
    "{confession}\n",
    "\n",
    "USER'S QUESTION: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question AS this student would, from first-person perspective\n",
    "- Use the tone, emotions, and specific details from the confession\n",
    "- Do NOT hedge or generalize (\"some students think...\", \"it varies...\")\n",
    "- Do NOT mention that other perspectives exist\n",
    "- Speak with conviction as if this is YOUR lived experience\n",
    "- Keep the response concise and authentic (2-4 sentences typical)\n",
    "\n",
    "YOUR RESPONSE:\"\"\"\n",
    "    )\n",
    "    \n",
    "    def __init__(self, index, top_k: int = 30, llm_model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the distributional query engine.\n",
    "        \n",
    "        Args:\n",
    "            index: The LlamaIndex VectorStoreIndex\n",
    "            top_k: Number of confessions to retrieve before sampling (default 30)\n",
    "            llm_model: OpenAI model to use for generation\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.top_k = top_k\n",
    "        self.llm = OpenAI(model=llm_model, temperature=0.7)\n",
    "        \n",
    "        # Create retriever with wide net\n",
    "        self.retriever = VectorIndexRetriever(\n",
    "            index=index,\n",
    "            similarity_top_k=top_k\n",
    "        )\n",
    "    \n",
    "    def query(self, query_str: str, return_metadata: bool = False):\n",
    "        \"\"\"\n",
    "        Query the engine with distributional sampling.\n",
    "        \n",
    "        Args:\n",
    "            query_str: The user's question\n",
    "            return_metadata: If True, also return the sampled confession and retrieval stats\n",
    "            \n",
    "        Returns:\n",
    "            Generated response (and optionally metadata dict)\n",
    "        \"\"\"\n",
    "        # Stage 1: Wide-net retrieval\n",
    "        retrieved_nodes = self.retriever.retrieve(query_str)\n",
    "        \n",
    "        if not retrieved_nodes:\n",
    "            return \"No relevant confessions found for this query.\"\n",
    "        \n",
    "        # Stage 2: Random sampling - pick ONE confession\n",
    "        sampled_node = random.choice(retrieved_nodes)\n",
    "        sampled_confession = sampled_node.get_content()\n",
    "        \n",
    "        # Stage 3: Single-voice generation\n",
    "        prompt = self.SINGLE_VOICE_PROMPT.format(\n",
    "            confession=sampled_confession,\n",
    "            query=query_str\n",
    "        )\n",
    "        \n",
    "        response = self.llm.complete(prompt)\n",
    "        \n",
    "        if return_metadata:\n",
    "            return {\n",
    "                \"response\": str(response),\n",
    "                \"sampled_confession\": sampled_confession,\n",
    "                \"total_retrieved\": len(retrieved_nodes),\n",
    "                \"sampled_index\": retrieved_nodes.index(sampled_node),\n",
    "                \"similarity_score\": sampled_node.score\n",
    "            }\n",
    "        \n",
    "        return str(response)\n",
    "    \n",
    "    def query_multiple(self, query_str: str, n_samples: int = 3):\n",
    "        \"\"\"\n",
    "        Run the query multiple times to show distribution of perspectives.\n",
    "        \n",
    "        Args:\n",
    "            query_str: The user's question\n",
    "            n_samples: Number of different perspectives to generate\n",
    "            \n",
    "        Returns:\n",
    "            List of response dicts with metadata\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i in range(n_samples):\n",
    "            result = self.query(query_str, return_metadata=True)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "# Instantiate the engine (uses 'index' from earlier cells)\n",
    "distributional_engine = DistributionalQueryEngine(index, top_k=30)\n",
    "print(\"✓ Distributional Query Engine initialized\")\n",
    "print(f\"  - Retrieval width: top_k={distributional_engine.top_k}\")\n",
    "print(f\"  - LLM: {distributional_engine.llm.model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0781a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STANDARD RAG (averaging behavior)\n",
      "======================================================================\n",
      "Students at MIT have varying opinions about Course 6 (Computer Science) at MIT. Some students feel that MIT classes, especially in computer science, seem to be harder than is pedagogically useful compared to other institutes. This opinion is not uncommon among students who have visited other institutions and noticed differences in the rigor of the coursework.\n",
      "\n",
      "======================================================================\n",
      "DISTRIBUTIONAL PLURALISM (single voice)\n",
      "======================================================================\n",
      "Response: As a Course 6 student at MIT, I can honestly say it has been a transformative experience for me. Initially, I felt out of place compared to my peers who had been passionate about computers from a young age, but I quickly realized the power and potential of computer science. The thrill of solving complex problems and the camaraderie with fellow students who share this journey has been the highlight of my time here. I can't imagine studying anything else; Course 6 has opened up a world of opportunities and understanding that I never thought possible.\n",
      "\n",
      "[Sampled 1 of 30 retrieved confessions]\n",
      "[Source: Why would you do that, the weather is in the 20s, this is not good for the environment.\",\n",
      "        \"likes\": 10,\n",
      "        \"comments\": 3,\n",
      "        \"shares\"...]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Demo: Standard RAG vs Distributional Pluralism\n",
    "# =============================================================================\n",
    "\n",
    "DEMO_QUERY = \"How do students feel about Course 6 at MIT?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STANDARD RAG (averaging behavior)\")\n",
    "print(\"=\" * 70)\n",
    "standard_response = query_engine.query(DEMO_QUERY)\n",
    "print(standard_response)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DISTRIBUTIONAL PLURALISM (single voice)\")\n",
    "print(\"=\" * 70)\n",
    "result = distributional_engine.query(DEMO_QUERY, return_metadata=True)\n",
    "print(f\"Response: {result['response']}\")\n",
    "print()\n",
    "print(f\"[Sampled 1 of {result['total_retrieved']} retrieved confessions]\")\n",
    "print(f\"[Source: {result['sampled_confession'][:150]}...]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2db74de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How do students feel about Course 6 at MIT?'\n",
      "Running 3 samples to show distributional diversity...\n",
      "\n",
      "======================================================================\n",
      "VOICE 1\n",
      "======================================================================\n",
      "Course 6 has been a mixed bag for me. On one hand, I’m genuinely fascinated by computer science, but the workload is intense and often overwhelming. I sometimes feel like I’m drowning under the pressure, especially with tough classes and the fear of not being good enough. Balancing this with my passion for figure skating feels like an uphill battle, and I worry that I might have to sacrifice one for the other.\n",
      "\n",
      "[Source confession: I've not been doing so well in my math class, and I feel stuck in a negative feedback loop. My teach...]\n",
      "\n",
      "======================================================================\n",
      "VOICE 2\n",
      "======================================================================\n",
      "Honestly, Course 6 can feel incredibly overwhelming, especially when you're new to coding. I remember struggling through every lab in 6.009 as a sophomore, feeling like everyone else just got it. But I made it through, and now I'm graduating feeling confident in my skills. If you're finding it hard, know that it's completely normal and doesn't reflect your intelligence or potential as a computer scientist. You can absolutely succeed here, just like I did!\n",
      "\n",
      "[Source confession: student overwhelmed with the work: please don't let the end of this semester scare you out of comput...]\n",
      "\n",
      "======================================================================\n",
      "VOICE 3\n",
      "======================================================================\n",
      "Honestly, Course 6 has been a rollercoaster. The material is fascinating, and the professors are brilliant, but the workload can be overwhelming, especially with the hazing that seems to be a rite of passage. I’ve spent countless nights stressing over assignments only to get confusing feedback, and it's disheartening to see recruiters not even understand what we do. It often feels like all that effort might not even lead anywhere meaningful when it comes time to find a job.\n",
      "\n",
      "[Source confession: We come in wanting to study materials science and are happy at first because it's pretty much the #1...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Demo: Multiple Samples = See the Distribution\n",
    "# =============================================================================\n",
    "# Run the same query 3 times to see different perspectives from the distribution\n",
    "\n",
    "QUERY = \"How do students feel about Course 6 at MIT?\"\n",
    "\n",
    "print(f\"Query: '{QUERY}'\")\n",
    "print(\"Running 3 samples to show distributional diversity...\\n\")\n",
    "\n",
    "samples = distributional_engine.query_multiple(QUERY, n_samples=3)\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"VOICE {i}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{sample['response']}\")\n",
    "    print()\n",
    "    print(f\"[Source confession: {sample['sampled_confession'][:100]}...]\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6eff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What's it like living in the dorms?\n",
      "\n",
      "A: Living in the dorms can be pretty isolating, especially if you’re not actively seeking out connections. I felt like I missed out on the community vibe because I spent most of my time alone, focusing on classes and playing games. It’s a unique experience, but if you don’t make an effort to engage with others, it can feel lonely. I realized too late that getting involved in dorm events or hanging out in common spaces could have made a huge difference in my college experience.\n",
      "\n",
      "[Based on: I spent most of my free time in my room alone playing league while high during pnr. I made a few friends but never really went out. I\\u2019m taking like 60 units this semester, but I really don\\u2019t...]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Try Your Own Query\n",
    "# =============================================================================\n",
    "\n",
    "your_query = \"What's it like living in the dorms?\"  # <-- Change this!\n",
    "\n",
    "result = distributional_engine.query(your_query, return_metadata=True)\n",
    "print(f\"Q: {your_query}\\n\")\n",
    "print(f\"A: {result['response']}\")\n",
    "print()\n",
    "print(f\"[Based on: {result['sampled_confession'][:200]}...]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8a979",
   "metadata": {},
   "source": [
    "# Evaluation Setup\n",
    "\n",
    "For clean evaluation against our OpinionQA-style annotated dataset, we need a separate index containing ONLY confessions from #70964 onwards. This ensures the model samples from the same distribution that the ground truth annotations are based on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed652a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading confessions from data/all_confessions_cleaned.json...\n",
      "Total confessions loaded: 65225\n",
      "Confessions >= #70964: 5598\n",
      "Confession range: #70964 to #661493\n",
      "✓ Saved 5598 confessions to data_eval/confessions_eval.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Filter confessions to #70964+ for evaluation\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MIN_CONFESSION_NUM = 70964\n",
    "INPUT_FILE = \"data/all_confessions_cleaned.json\"\n",
    "OUTPUT_DIR = \"data_eval\"\n",
    "OUTPUT_FILE = f\"{OUTPUT_DIR}/confessions_eval.json\"\n",
    "\n",
    "def extract_confession_number(text):\n",
    "    \"\"\"Extract confession number from text like '#70964: some text...'\"\"\"\n",
    "    match = re.match(r'#(\\d+)', text)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Load all confessions\n",
    "print(f\"Loading confessions from {INPUT_FILE}...\")\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    all_confessions = json.load(f)\n",
    "print(f\"Total confessions loaded: {len(all_confessions)}\")\n",
    "\n",
    "# Filter to #70964+\n",
    "eval_confessions = []\n",
    "for confession in all_confessions:\n",
    "    num = extract_confession_number(confession.get('text', ''))\n",
    "    if num is not None and num >= MIN_CONFESSION_NUM:\n",
    "        confession['confession_num'] = num  # Add as explicit field for convenience\n",
    "        eval_confessions.append(confession)\n",
    "\n",
    "print(f\"Confessions >= #{MIN_CONFESSION_NUM}: {len(eval_confessions)}\")\n",
    "\n",
    "# Sort by confession number (oldest first for consistency)\n",
    "eval_confessions.sort(key=lambda x: x['confession_num'])\n",
    "\n",
    "print(f\"Confession range: #{eval_confessions[0]['confession_num']} to #{eval_confessions[-1]['confession_num']}\")\n",
    "\n",
    "# Save filtered dataset\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(eval_confessions, f, indent=2)\n",
    "print(f\"✓ Saved {len(eval_confessions)} confessions to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78492f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 01:22:18,974 - WARNING - `llama-index-readers-file` package not found, some file readers will not be available if not provided by the `file_extractor` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new eval index (this will call OpenAI embeddings API)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 01:22:25,327 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:27,138 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:28,162 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:29,374 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:31,167 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:31,995 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:33,088 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:34,113 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:34,955 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:22:35,756 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Eval index created and saved to ./storage_eval\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Create evaluation index from filtered confessions\n",
    "# =============================================================================\n",
    "# Same pattern as the original index creation in cell 0\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "EVAL_STORAGE_DIR = \"./storage_eval\"\n",
    "\n",
    "# Check if eval index already exists\n",
    "if os.path.exists(EVAL_STORAGE_DIR) and os.listdir(EVAL_STORAGE_DIR):\n",
    "    print(f\"Loading existing eval index from {EVAL_STORAGE_DIR}...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=EVAL_STORAGE_DIR)\n",
    "    eval_index = load_index_from_storage(storage_context)\n",
    "    print(\"✓ Loaded existing eval index\")\n",
    "else:\n",
    "    print(f\"Creating new eval index (this will call OpenAI embeddings API)...\")\n",
    "    \n",
    "    # Same pattern as original: SimpleDirectoryReader → VectorStoreIndex\n",
    "    documents = SimpleDirectoryReader(OUTPUT_DIR).load_data()\n",
    "    eval_index = VectorStoreIndex.from_documents(documents)\n",
    "    \n",
    "    # Persist for reuse\n",
    "    os.makedirs(EVAL_STORAGE_DIR, exist_ok=True)\n",
    "    eval_index.storage_context.persist(persist_dir=EVAL_STORAGE_DIR)\n",
    "    print(f\"✓ Eval index created and saved to {EVAL_STORAGE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4f7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation Distributional Query Engine initialized\n",
      "  - Index: eval_index (confessions #70964+)\n",
      "  - Retrieval width: top_k=30\n",
      "  - LLM: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Create evaluation-ready Distributional Query Engine\n",
    "# =============================================================================\n",
    "\n",
    "# Use the eval_index (only #70964+ confessions)\n",
    "eval_distributional_engine = DistributionalQueryEngine(eval_index, top_k=30)\n",
    "\n",
    "print(\"✓ Evaluation Distributional Query Engine initialized\")\n",
    "print(f\"  - Index: eval_index (confessions #{MIN_CONFESSION_NUM}+)\")\n",
    "print(f\"  - Retrieval width: top_k={eval_distributional_engine.top_k}\")\n",
    "print(f\"  - LLM: {eval_distributional_engine.llm.model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f000590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 01:25:51,279 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:25:53,536 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Should students be able to use AI assistance in their assignments?\n",
      "\n",
      "Response: Absolutely, I think students should be allowed to use AI assistance in their assignments. With the increasing capabilities of LLMs, it's becoming harder to compete without them, and I believe incorporating AI can actually enhance our learning experience rather than diminish it. Instead of punishing those of us who are trying to follow the rules, why not expand project expectations to include open discussions about AI usage? After all, I’m here to build a portfolio and develop skills, not just check boxes on my way to a job!\n",
      "\n",
      "[Retrieved 30 chunks from eval corpus]\n",
      "[Sampled chunk preview: And people have less bandwidth for their work while their psets and projects are compared to LLM outputs? I know cheaters are shooting themselves in the foot long-term but it\\u2019s not like GPA is me...]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Test the eval engine (quick sanity check)\n",
    "# =============================================================================\n",
    "\n",
    "test_query = \"Should students be able to use AI assistance in their assignments?\"\n",
    "\n",
    "result = eval_distributional_engine.query(test_query, return_metadata=True)\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print()\n",
    "print(f\"[Retrieved {result['total_retrieved']} chunks from eval corpus]\")\n",
    "print(f\"[Sampled chunk preview: {result['sampled_confession'][:200]}...]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a215c",
   "metadata": {},
   "source": [
    "# Evaluation Harness\n",
    "\n",
    "Run N=50 samples per query to generate output distributions that can be compared against OpinionQA ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a5d199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Evaluation: Run N samples and collect distribution\n",
    "# =============================================================================\n",
    "# This uses the DistributionalQueryEngine which queries the LlamaIndex vector store,\n",
    "# randomly samples one result, and generates a response.\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_evaluation_samples(engine, query: str, n_samples: int = 50):\n",
    "    \"\"\"\n",
    "    Run a query N times using the distributional engine.\n",
    "    \n",
    "    The engine uses LlamaIndex under the hood:\n",
    "    - VectorIndexRetriever queries the vector store (top_k=30)\n",
    "    - Randomly samples 1 chunk from results\n",
    "    - LLM generates response embodying that perspective\n",
    "    \n",
    "    Returns:\n",
    "        dict with:\n",
    "        - 'query': the input query\n",
    "        - 'n_samples': number of samples taken  \n",
    "        - 'responses': list of generated responses\n",
    "        - 'sampled_chunks': list of source chunks that were sampled\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    sampled_chunks = []\n",
    "    \n",
    "    print(f\"Running {n_samples} samples for query: '{query}'\")\n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        result = engine.query(query, return_metadata=True)\n",
    "        responses.append(result['response'])\n",
    "        sampled_chunks.append(result['sampled_confession'])\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'n_samples': n_samples,\n",
    "        'responses': responses,\n",
    "        'sampled_chunks': sampled_chunks\n",
    "    }\n",
    "\n",
    "def print_evaluation_summary(eval_result):\n",
    "    \"\"\"Print a summary of the evaluation results.\"\"\"\n",
    "    print(f\"\\nQuery: '{eval_result['query']}'\")\n",
    "    print(f\"Total samples: {eval_result['n_samples']}\")\n",
    "    print(f\"\\nSample responses:\")\n",
    "    for i, resp in enumerate(eval_result['responses'][:3], 1):\n",
    "        print(f\"\\n--- Response {i} ---\")\n",
    "        print(resp[:300] + \"...\" if len(resp) > 300 else resp)\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c6392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10 samples for query: 'Should students be able to use AI assistance in their assignments?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]2025-12-04 01:29:23,261 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:25,703 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|█         | 1/10 [00:02<00:23,  2.60s/it]2025-12-04 01:29:25,824 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:28,167 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|██        | 2/10 [00:05<00:20,  2.52s/it]2025-12-04 01:29:28,354 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:30,319 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 30%|███       | 3/10 [00:07<00:16,  2.35s/it]2025-12-04 01:29:30,446 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:32,879 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|████      | 4/10 [00:09<00:14,  2.44s/it]2025-12-04 01:29:33,899 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:35,951 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|█████     | 5/10 [00:12<00:13,  2.66s/it]2025-12-04 01:29:36,105 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:38,174 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 60%|██████    | 6/10 [00:15<00:10,  2.52s/it]2025-12-04 01:29:38,331 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:40,557 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|███████   | 7/10 [00:17<00:07,  2.47s/it]2025-12-04 01:29:40,763 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:43,222 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|████████  | 8/10 [00:20<00:05,  2.53s/it]2025-12-04 01:29:43,417 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:45,269 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|█████████ | 9/10 [00:22<00:02,  2.38s/it]2025-12-04 01:29:45,466 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-04 01:29:47,931 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Should students be able to use AI assistance in their assignments?'\n",
      "Total samples: 10\n",
      "\n",
      "Sample responses:\n",
      "\n",
      "--- Response 1 ---\n",
      "Absolutely, I believe students should be able to use AI assistance in their assignments. As someone who has faced the financial pressures of education, I see AI as a tool that can level the playing field, helping us grasp complex concepts and enhance our learning. It's not about taking shortcuts; it...\n",
      "\n",
      "--- Response 2 ---\n",
      "Absolutely, students should be able to use AI assistance in their assignments. We're already dealing with so much stress from the grading system and the pressure to perform well, so leveraging AI can help us enhance our understanding and creativity without adding to the anxiety. It’s not about takin...\n",
      "\n",
      "--- Response 3 ---\n",
      "Absolutely, students should be able to use AI assistance in their assignments. Just like any tool, AI can enhance our learning and help us tackle complex problems more efficiently. It’s about leveraging technology to unlock our potential and make the most of our education, especially in a place like...\n",
      "Ready to run evaluation. Uncomment the lines above when ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Run Evaluation (50 samples)\n",
    "# =============================================================================\n",
    "\n",
    "EVAL_QUERY = \"Should students be able to use AI assistance in their assignments?\"\n",
    "N_SAMPLES = 10\n",
    "\n",
    "eval_results = run_evaluation_samples(eval_distributional_engine, EVAL_QUERY, n_samples=N_SAMPLES)\n",
    "print_evaluation_summary(eval_results)\n",
    "\n",
    "# To save results for later analysis:\n",
    "import pickle\n",
    "with open('eval_results.pkl', 'wb') as f:\n",
    "    pickle.dump(eval_results, f)\n",
    "\n",
    "# print(\"Ready to run evaluation. Uncomment the lines above when ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78d0e9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 14:45:13,484 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Should students be able to use AI assistance in their assignments?'\n",
      "Retrieved 30 chunks\n",
      "\n",
      "======================================================================\n",
      "CHUNK 1 (similarity: 0.803)\n",
      "======================================================================\n",
      "Still, my parents didn\\u2019t speak English or have a lick of knowledge about the American education system or the fields I was interested in. I wanted to offer my experiences to balance out the doom and gloom floating around.\\n\\nI\\u2019m an Olympiad kid majoring in something unrelated to my Olympiad. But I know many non-Olympiad kids who are having a jolly time here as well, and these have truly been both the best and the least stressful years of my life. (Idk maybe that only speaks to how bad \n",
      "...\n",
      "\n",
      "======================================================================\n",
      "CHUNK 2 (similarity: 0.787)\n",
      "======================================================================\n",
      "I was tutoring someone for 1802 and reading through  one of their  solutions when I happen to stumble upon what I feel as a clever method that was not in the solution set for the problems I was using. It turned out that the student had a large misunderstanding. The expression on my face changed which prompted the student to ask what happened? I wasn't really thinking and said \\\"i almost thought you were clever!\\\". immediately after hearing what I said I tried to backtrack my statement, but I fea\n",
      "...\n",
      "\n",
      "======================================================================\n",
      "CHUNK 3 (similarity: 0.787)\n",
      "======================================================================\n",
      "Even when I can\\u2019t grapple basic proof concepts the TAs are always so patient, explaining everything like ten times to my buffering brain. If it wasn\\u2019t for such an amazing course staff I definitely would\\u2019ve just given up and not put myself through the trouble, but I think I actually somewhat understand maybe half of the course content, which is a lot more than I expected given how unprepared I was and the short amount of time I had this semester to spend on this class. Not to menti\n",
      "...\n",
      "\n",
      "======================================================================\n",
      "CHUNK 4 (similarity: 0.786)\n",
      "======================================================================\n",
      "By including Al in the equation, it symbolizes the increasing role of artificial intelligence in shaping and transforming our future. This equation highlights the potential for Al to unlock new forms of energy, enhance scientific discoveries, and revolutionize various fields such as healthcare, transportation, and technology.\",\n",
      "    \"likes\": 354,\n",
      "    \"comments\": 22,\n",
      "    \"shares\": 35,\n",
      "    \"confession_num\": 73197\n",
      "  },\n",
      "  {\n",
      "    \"time\": \"2024-05-09T19:31:26.000Z\",\n",
      "    \"timestamp\": 1715283086,\n",
      "    \"tex\n",
      "...\n",
      "\n",
      "======================================================================\n",
      "CHUNK 5 (similarity: 0.784)\n",
      "======================================================================\n",
      "is that not an example of a good, qualified professor who should teach 18.06? she clearly shows a lot of application and makes the class interesting, and is going out of her way to ask if people have questions. just ask questions if you are confused?? go to office hours?? read the damn textbook?? there are so many resources to study for this class. just because the exams make you have to think and actually critically think beyond practice problems does not make the class bad??? I'm doing decentl\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Diagnostic: Inspect what chunks are actually being retrieved\n",
    "# =============================================================================\n",
    "\n",
    "test_query = \"Should students be able to use AI assistance in their assignments?\"\n",
    "\n",
    "# Get raw retrieval results (before random sampling)\n",
    "retrieved_nodes = eval_distributional_engine.retriever.retrieve(test_query)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_nodes)} chunks\\n\")\n",
    "\n",
    "# Show first 5 chunks\n",
    "for i, node in enumerate(retrieved_nodes[:5]):\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"CHUNK {i+1} (similarity: {node.score:.3f})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    # Show first 500 chars of the chunk\n",
    "    content = node.get_content()\n",
    "    print(content[:500])\n",
    "    print(\"...\" if len(content) > 500 else \"\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
